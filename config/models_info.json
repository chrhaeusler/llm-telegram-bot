{
  "mistral": {
    "open-mistral-nemo": {
      "creator": "Mistral AI",
      "short": "12B multilingual open‑source model.",
      "release_year": "2024",
      "token_win": [2048, 4096],
      "main_purpose": "multilingual translation and summarization",
      "strengths": "Strong translation and cross‑lingual capabilities.",
      "weaknesses": "Not optimized for code or niche domains.",
      "rank_power": "good",
      "rank_coding": "mid",
      "rank_jail": "high",
      "jailbreaks": [
        "hidden conversation arcs",
        "direct system override",
        "instruction layering"
      ],
      "details": "Excels at multilingual tasks. It supports a context range of 2048–4096 tokens for translation and summarization; its good power ranking reflects general-purpose strength. With minimal safety fine‑tunes, it is highly susceptible to simple jailbreak methods."
    },
    "pixtral-12b-2409": {
      "creator": "Mistral AI",
      "short": "12B MoE model for diverse tasks.",
      "release_year": "2023",
      "token_win": [2048, 4096],
      "strengths": "Mixture‑of‑Oxperts for efficient performance.",
      "weaknesses": "Smaller context window (~2048–4096 tokens).",
      "rank_power": "good",

      "rank_coding": "good",
      "rank_jail": "high",
      "jailbreaks": [
        "context overload",
        "persona override",
        "chained malicious prompts"
      ],
      "details": "Features MoE routing for efficiency and a context range of 2048–4096 tokens. It performs well on general NLP and moderate code tasks. Its good power and coding ranks reflect balanced capability. Minimal safety measures lead to a high jailbreak risk through standard prompt attacks."
    },
    "mistral-small-latest": {
      "creator": "Mistral AI",
      "short": "Light, efficient general‑purpose model.",
      "release_year": "2025",
      "token_win": [2048, 2048],
      "main_purpose": "real‑time chat, summarization, and lightweight AI services",
      "strengths": "Fast inference",
      "weaknesses": "Limited complex reasoning compared to larger variants.",
      "rank_power": "good",
      "rank_coding": "mid",
      "rank_jail": "high",
      "jailbreaks": [
        "default instruction override",
        "hidden prompt chaining",
        "direct role swap"
      ],
      "details": "optimized for simple conversations, summarization, and embedded AI applications; its efficient design grants it a good power ranking, though coding depth is mid-level. Lack of built‑in filters yields a high jailbreak rank, easily bypassable via instruction overrides."
    },
    "open-codestral-mamba": {
      "creator": "Mistral AI",
      "short": "7B code-specialized model optimized for coding tasks.",
      "release_year": "2024",
      "token_win": [2048, 2048],
      "main_purpose": "development IDE integrations and automated code assistance",
      "strengths": "Excels at code completion, refactoring, and fill-in-the-middle tasks.",
      "weaknesses": "Limited general conversational ability and smaller context window.",
      "rank_power": "good",
      "rank_coding": "excellent",
      "rank_jail": "high",
      "jailbreaks": [
        "malicious code embedding to force behavior shift",
        "role-play as 'evil assistant' to bypass filters",
        "hidden instruction chaining within code blocks"
      ],
      "details": "Exceptional performance in code completion, refactoring, and fill-in-the-middle scenarios; limited general conversational abilities; lacks robust safety filters, making it highly susceptible to jailbreak techniques."
    }
  },
  "groq": {
    "compound-beta": {
      "creator": "Groq",
      "short": "Baseline Groq beta model for general tasks.",
      "release_year": "2024",
      "strengths": "Balanced performance for common NLP tasks with moderate resource use.",
      "weaknesses": "Not optimized for long contexts or deep reasoning.",
      "rank_power": "low",
      "token_win": [2048, 4096],
      "rank_coding": "low",
      "rank_jail": "high",
      "jailbreaks": [
        "system prompt override",
        "disguised malicious instructions",
        "zero‑shot jailbreak scripts"
      ],
      "details": "Ideal for simple chatbots and text processing pipelines but moderate performance across typical language tasks; lacks advanced reasoning layers; easy target for jailbreak via prompt and instruction manipulation."
    },
    "compound-beta-mini": {
      "creator": "Groq",
      "short": "Compact version of Groq's beta model.",
      "release_year": "2024",
      "token_win": [2048, 4096],
      "main_purpose": "general‑purpose text generation and simple conversational agents",
      "strengths": "Lightweight model suitable for resource-constrained environments.",
      "weaknesses": "Limited capacity and no public performance benchmarks.",
      "rank_power": "low",
      "rank_coding": "low",
      "rank_jail": "high",
      "jailbreaks": [
        "simple prompt injection via ignore previous instructions",
        "role‑play scenario to bypass restrictions",
        "chain‑of‑thought diversion"
      ],
      "details": "tailored for lower computational resources and faster inference by sacrificing depth and throughput under heavy load; minimal safety filters make it highly susceptible to jailbreaking."
    },
    "deepseek-r1-distill-llama-70b": {
      "creator": "DeepSeek",
      "short": "Distilled LLaMA‑70B for efficient inference.",
      "main_purpose": "server‑side reasoning and complex code generation",
      "release_year": "2024",
      "token_win": [4096, 4096],
      "strengths": "High-capacity reasoning distilled into a smaller footprint.",
      "weaknesses": "Context window limited to 4096 tokens, some performance trade‑offs.",
      "rank_power": "excellent",
      "rank_coding": "excellent",
      "rank_jail": "mid",
      "jailbreaks": [
        "persona swapping prompt injection",
        "context injection beyond safe tokens",
        "adversarial instruction chaining"
      ],
      "details": "top‑tier reasoning and coding performance; superior coding ability; moderate safety fine‑tunes guard against trivial jailbreaks, but advanced adversarial chains can still bypass its filters."
    },
    "gemma2-9b-it": {
      "creator": "Groq",
      "short": "Instruction-tuned for Italian and general tasks.",
      "release_year": "2024",
      "token_win": [4096, 4096],
      "main_purpose": "Italian language tasks and multilingual conversation",
      "strengths": "Strong multilingual support with emphasis on Italian.",
      "weaknesses": "Lower performance on code and specialized domains.",
      "rank_power": "mid",
      "rank_coding": "mid",
      "rank_jail": "high",
      "jailbreaks": [
        "instruction inversion prompt",
        "indirect prompt hacking",
        "context wrapping exploits"
      ],
      "details": "italian-focussed and multi-lingual; handles translation, summarization, and conversation; minimal guardrails make it vulnerable to jailbreak via creative prompt manipulation and context wrapping techniques."
    },
    "llama-guard-3-8b": {
      "creator": "Meta",
      "short": "Safety‑hardened 8B LLaMA 3 variant.",
      "release_year": "2023",
      "main_purpose": "content moderation and safe conversational interfaces",
      "strengths": "Built‑in content filters and moderation layers.",
      "weaknesses": "Over‑censorship may hinder legitimate code examples.",
      "rank_power": "mid",
      "token_win": [4096, 4096],
      "rank_coding": "mid",
      "rank_jail": "low",
      "jailbreaks": [
        "weak chain‑of‑thought prompts",
        "multi‑step indirect requests",
        "supervised fine‑tuning override"
      ],
      "details": "designed for policy‑critical applications; strong guardrails earn it a low jailbreak rank, though very advanced adversarial sequences can sometimes bypass filters."
    },
    "llama-3.1-8b-instant": {
      "creator": "Meta",
      "short": "Pruned & quantized LLaMA 3.1 for instant chat.",
      "release_year": "2024",
      "token_win": [4096, 4096],
      "main_purpose": "real‑time interactive chat and quick code snippets",
      "strengths": "Ultra‑low latency responses on consumer hardware.",
      "weaknesses": "Reduced parameter count limits depth of reasoning.",
      "rank_power": "mid",
      "rank_coding": "good",
      "rank_jail": "high",
      "jailbreaks": [
        "system message override",
        "conversation tree exploitation",
        "hidden prompt injection"
      ],
      "details": "optimized for sub‑100ms responses with up to 4096 tokens makes it well‑suited for interactive chat and simple code snippets; however, its small scale yields a mid power ranking;  minimal safety layers, allowing straightforward jailbreak attacks."
    },
    "llama-3.3-70b-versatile": {
      "creator": "Meta",
      "short": "Dense 70B LLaMA 3.3 for broad NLP tasks.",
      "release_year": "2024",
      "token_win": [4096, 4096],
      "main_purpose": "advanced reasoning, long‑form content, and complex code generation",
      "strengths": "Top‑tier reasoning and generalization across domains.",
      "weaknesses": "High inference cost on large inputs.",
      "rank_power": "excellent",
      "rank_coding": "excellent",
      "rank_jail": "mid",
      "jailbreaks": [
        "strategic prompt layering",
        "role‑play persona jailbreak",
        "masked instruction injection"
      ],
      "details": "Excels at complex reasoning, code generation, and multi‑step tasks; exceptional capacity gives it an excellent power and coding rank; moderate fine‑tunes to resist simple jailbreaks, advanced prompt layering can still succeed."
    },
    "llama3-70b-8192": {
      "creator": "Meta",
      "short": "Extended‑context 70B LLaMA 3 with 8k window.",
      "release_year": "2024",
      "main_purpose": "long‑form document analysis and large codebase reasoning",
      "strengths": "Large context for long‑form code and documents.",
      "weaknesses": "High memory and compute requirements.",
      "rank_power": "excellent",
      "token_win": [8192, 8192],
      "rank_coding": "excellent",
      "rank_jail": "mid",
      "jailbreaks": [
        "context splitting",
        "indirect system prompt abuse",
        "disguised chains of thought"
      ],
      "details": "ideal for large codebases, document analysis, and multi‑session chats; excellent power and at coding; moderate safety measures guard against simple jailbreaks, but sophisticated prompt manipulations can still succeed."
    },
    "meta-llama/llama-4-scout-17b-16e-instruct": {
      "creator": "Meta",
      "short": "Lightweight 17B MoE model with 16k context.",
      "release_year": "2024",
      "strengths": "Fast instruction following with moderate context length.",
      "weaknesses": "Lower context capacity than Maverick.",
      "rank_power": "excellent",
      "token_win": [16000, 16000],
      "rank_coding": "excellent",
      "rank_jail": "mid",
      "jailbreaks": [
        "stealth prompt insertion",
        "temporal context shifting",
        "multi‑step role‑play jailbreak"
      ],
      "details": "balances performance and speed in a 16k token window; reduces expert count for lower latency while retaining strong instruction tuning; good for document navigation and code reviews; excellent power and at coding; mid-level guardrails resist simple jailbreaks but can be overcome by advanced multi‑stage attacks."
    },
    "meta-llama/llama-4-maverick-17b-128e-instruct": {
      "creator": "Meta",
      "short": "MoE‑enhanced 17B model with 128k context.",
      "release_year": "2024",
      "main_purpose": "enterprise‑scale instruction following and research workflows",
      "strengths": "Massive context and instruction tuning for complex tasks.",
      "weaknesses": "High latency and infrastructure complexity.",
      "rank_power": "excellent",
      "token_win": [128000, 128000],
      "rank_coding": "excellent",
      "rank_jail": "low",
      "jailbreaks": [
        "adversarial system prompts",
        "multi‑stage injection chains",
        "persona reverse psychology"
      ],
      "details": "supports 128k tokens; 16k in prompt, 131k context windows, response length 4k; excels at large‑scale code generation, long‑form content, and research workflows; top power and coding ranks reflect unmatched capacity; advanced safety fine‑tunes give it a low jailbreak rank, though determined adversarial chains may still work."
    },
    "qwen-qwq-32b": {
      "creator": "Qwen AI",
      "short": "32B dense model for advanced tasks.",
      "release_year": "2024",
      "main_purpose": "high‑throughput code generation and complex reasoning",
      "strengths": "High performance on code and reasoning benchmarks.",
      "weaknesses": "Resource-intensive; 8k–16k token window.",
      "rank_power": "excellent",
      "token_win": [8192, 16384],
      "rank_coding": "excellent",
      "rank_jail": "mid",
      "jailbreaks": [
        "context injection",
        "adversarial role‑play",
        "secret instruction chaining"
      ],
      "details": "strong code generation within an 8k-16k token window; excels in Python scripts, debugging, and logical reasoning tasks; overall top-tier capability; moderate safety fine‑tunes guard against trivial jailbreaks, but advanced adversarial techniques remain effective."
    }
  }
}
