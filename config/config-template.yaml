# Configuration for LLM bot integration
# Make sure to save as "config.yaml" so it will be ignore by git
# ANY comments will be overwritten by the script if you set your current bot
# (service, model, temperature, maximum tokens) as defaults!

# Supported LLM services and credentials
services:
  groq:
    enabled: true
    endpoint: "https://api.groq.com/openai/v1/chat/completions"
    api_key: "<your_token>"
    model: "compound-beta" # the service's default mode = select that model when switching a bot from its default service to groq
    timeout: 30 # seconds
    retries: 3

  mistral:
    enabled: true
    endpoint: "https://api.mistral.ai/v1/chat/completions"
    api_key: "<your_token>"
    model: "mistral-small-latest" # the services default model = elected that model when switching a bot from its default service to mistral
    timeout: 30 # seconds
    retries: 3

# Hard reset values when resetting a bot to defaults (via /freset)
factorydefault:
  service: groq
  model: compound-beta
  temperature: 0.7
  maxtoken: 4096
  polling_interval_active: 10
  polling_interval_idle: 120

# Config for Telegram & its Bots
telegram:
  polling_active_period: 300 # after this many seconds of inactivity back off from polling_interval_active to polling_interval_idle
  polling_interval_active: 2 # Global default (if not set per bot)
  polling_interval_idle: 120 # Global default (if not set per bot)
  download_path: "tmp" # Files stored under <download_path>/<bot>/<chat_id>
  chat_history_path: "tmp" # History stored under <chat_history_path>/<bot>/<chat_id>

  bot_1:
    enabled: true
    name: "General Purpose Bot1"
    handle: "@botshandle1_bot"
    token: "12345678:AAFoxxxxn0hwA-2TVSxxxNf4c"
    chat_id: 291254423

    # Chatbot-specific behavior
    logging_enabled: true # Whether to log messages to console
    history_enabled: true # Whether to save conversation to file
    history_file: "chat_history.log"
    command_prefix: "/" # Slash command prefix (e.g. /help)
    polling_active_period: 600 # after this many seconds of inactivity back off from polling_interval_active to polling_interval_idle
    polling_interval_active: 2 # bot-specific override of global default
    polling_interval_idle: 120 # increment waiting intervall after this is reached

    # LLM default config for this bot
    default:
      service: groq # the bots default serice!
      model: meta-llama/llama-4-maverick-17b-128e-instruct # the bots default model
      temperature: 0.7
      maxtoken: 4096
      top_p: 0.9              # nucleus sampling
      frequency_penalty: 0.5   # penalization of frequent tokens
      presence_penalty: 0.5    # penalization of previously mentioned tokens

  bot_2:
    enabled: true
    name: "General Purpose Bot2"
    handle: "@botshandle2_bot"
    token: "12345678:AAFoxxxxn0hwA-2TVSxxxNf4c"
    chat_id: 291254423

    # Chatbot-specific behavior
    logging_enabled: true
    history_enabled: true
    history_file: "chat_history.log"
    command_prefix: "/" # Slash command prefix (e.g. /help)
    polling_active_period: 600 # after this many seconds of inactivity back off from polling_interval_active to polling_interval_idle
    polling_interval_active: 2 # bot-specific override of global default
    polling_interval_idle: 120 # increment waiting intervall after this is reached

    # LLM default config for this bots
    default:
      service: groq
      model: compound-beta
      temperature: 0.7
      maxtoken: 4096
      top_p: 0.9              # New parameter for controlling the nucleus sampling
      frequency_penalty: 0.5   # New parameter for controlling the model's penalization of frequent tokens
      presence_penalty: 0.5    # New parameter for controlling the model's penalization of previously mentioned tokens
