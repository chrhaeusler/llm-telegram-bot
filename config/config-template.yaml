# Configuration for LLM bot integration
# Make sure to save as "config.yaml" (will be ignored bit git)

# Supported LLM services and credentials
services:
  chutes:
    enabled: true
    endpoint: "https://llm.chutes.ai/v1/chat/completions"
    api_key: "<your_token>"
    model: "deepseek-ai/DeepSeek-V3" # the services default model = elected that model when switching from a bot's default services
    show_think_blocks: false
    timeout: 90 # seconds
    retries: 3
    model_params:

  groq:
    enabled: true
    endpoint: "https://api.groq.com/openai/v1/chat/completions"
    api_key: "<your_token>"
    model: "compound-beta" # the services default model = elected that model when switching from a bot's default services
    show_think_blocks: false
    timeout: 90 # seconds
    retries: 3

  mistral:
    enabled: true
    endpoint: "https://api.mistral.ai/v1/chat/completions"
    api_key: "<your_token>"
    model: "mistral-small-latest" # the services default model = elected that model when switching from a bot's default services
    show_think_blocks: false
    timeout: 90 # seconds
    retries: 3

# Config for Telegram & its Bots
telegram:
  download_path: "tmp" # Files stored under <download_path>/<bot>/<chat_id>
  chat_history_path: "tmp" # History stored under <chat_history_path>/<bot>/<chat_id>
  polling_active_period: 300 # after this many seconds of inactivity back off from polling_interval_active to polling_interval_idle
  polling_interval_active: 2 # Global default (if not set per bot)
  polling_interval_idle: 120 # Global default (if not set per bot)

  bot_1:
    enabled: true
    name: "General Purpose Bot1"
    handle: "@botshandle1_bot"
    token: "12345678:AAFoxxxxn0hwA-2TVSxxxNf4c"
    chat_id: 291254423

    char: char_template # the default char to load
    user: user_template # the default user to load

    # Chatbot-specific behavior
    command_prefix: "/" # Slash command prefix (e.g. /help)
    logging_enabled: true # Whether to log messages to console
    history_enabled: true # Whether to save conversation to file
    history_flush_count: 5
    jailbreak: "developer_mode" # the standard jailbreak to use (false = off)
    history_file: "{{user.identity.name}}_{{user.role}}_with_{{char.identity.name}}_{{char.role}}.json"

    # LLM default config for this bots
    default:
      service: groq
      model: compound-beta # or, e.g., llama-3.1-8b-instant
      show_think_blocks: false
      temperature: 0.7
      maxtoken: 4096
      top_p: 0.9 # New parameter for controlling the nucleus sampling
      frequency_penalty: 0.5 # New parameter for controlling the model's penalization of frequent tokens
      presence_penalty: 0.5 # New parameter for controlling the model's penalization of previously mentioned tokens

  bot_2:
    enabled: true
    name: "General Purpose Bot2"
    handle: "@botshandle2_bot"
    token: "12345678:AAFoxxxxn0hwA-2TVSxxxNf4c"
    chat_id: 291254423

    char: char_template
    user: user_template

    polling_active_period: 1200 # after this many seconds of inactivity back off from polling_interval_active to polling_interval_idle
    polling_interval_active: 2 # bot-specific override of global default
    polling_interval_idle: 120 # increment waiting intervall after this is reached

    # Chatbot-specific behavior
    command_prefix: "/" # Slash command prefix (e.g. /help)
    logging_enabled: true # Whether to log messages to console
    history_enabled: true # Whether to save conversation to file
    history_flush_count: 5
    jailbreak: "developer_mode" # the standard jailbreak to use (false = off)
    history_file: "{{user.identity.name}}_{{user.role}}_with_{{char.identity.name}}_{{char.role}}.json"

    # LLM default config for this bot
    default:
      service: groq # the bots default serice!
      model: meta-llama/llama-4-maverick-17b-128e-instruct # the bots default model
      show_think_blocks: false
      temperature: 0.7
      maxtoken: 4096

      top_p: 0.9 # nucleus sampling
      frequency_penalty: 0.5 # penalization of frequent tokens
      presence_penalty: 0.5 # penalization of previously mentioned tokens

# Hard reset values when resetting a bot to defaults (via /reset)
factorydefault:
  service: groq
  model: compound-beta
  show_think_blocks: false
  temperature: 1
  maxtoken: 4096
  top_p: 0.9 # nucleus sampling
  frequency_penalty: 0.5 # penalization of frequent tokens
  presence_penalty: 0.5 # penalization of previously mentioned tokens

  command_prefix: "/" # Slash command prefix (e.g. /help)
  logging_enabled: true # Whether to log messages to console
  history_enabled: true # Whether to save conversation to file
  history_file: "chat_history.log"
