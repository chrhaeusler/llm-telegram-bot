# Configuration for LLM bot integration
# Make sure to save as "config.yaml" so it will be ignore by git
# ANY comments will be overwritten by the script if you set your current bot
# (service, model, temperature, maximum tokens) as defaults!
factorydefault:
  service: groq
  model: compound-beta
  temperature: 0.7
  maxtoken: 1000
default:
  service: llama-3.1-8b-instant
  model: mistral-small-latest
  temperature: 0.7
  maxtoken: 1000
services:
  groq:
    enabled: true
    model: compound-beta
    endpoint: https://api.groq.com/openai/v1/chat/completions
    token_ref: <your_token>
  mistral:
    enabled: true
    model: mistral-small-latest
    endpoint: https://api.mistral.ai/v1/chat/completions
    token_ref: <your_token>
telegram:
  bot_token: 12345678:AAFoxxxxn0hwA-2TVSxxxNf4c
  chat_id:
    - 123456789 # Your Telegram user ID, for security
  polling_interval_idle: 120 # seconds
  polling_interval_active: 1 # seconds
